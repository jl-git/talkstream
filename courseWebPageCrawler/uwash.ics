BEGIN:VCALENDAR
VERSION:2.0
PRODID:Data::ICal 0.20
X-LIC-LOCATION:America/Los_Angeles
X-WR-CALNAME:UW CSE Colloquium Calendar
X-WR-TIMEZONE:America/Los_Angeles
BEGIN:VEVENT
DESCRIPTION:Biomedical research is generating a huge amount of genomic data. Extracting knowledge from Big Data to guide novel biological discovery is critical to fully realize its potential. However\, high-dimensional genomic data is known for its heterogeneity and noise.  One important way to enrich for true signal is to examine its effects in different but complementary data types (e.g.\, mRNA expression\, histone modification\, and metabolite abundance). In this talk\, I will present my work on integrating multiple types of data to study how cellular metabolic\, transcriptional and epigenetic states interact in stem cell biology. First I will present a computational method that integrates gene expression\, metabolite abundance and flux\, and network topology to infer genome-scale cell type-specific metabolic networks. Then I will present a case study where the integration of gene expression and metabolite abundance data leads to the discovery of a metabolic enzyme controlling the level of repressive histone modifications\, which in turn regulate a set of genes critical for early human development. I will also present a new approach to identify genes that regulate lineage-specific differentiation by integrating gene expression and histone modification ChIP-seq data. In the future\, leveraging large genomic datasets from the ENCODE project\, NIH Roadmap Epigenomics project and the FANTOM5 project\, cell type-specific metabolic networks will be reconstructed as a framework to understand how developmental transcription factors affect metabolism\, the pattern of epigenetic modifications in the metabolic network\, and critical nodes in the metabolic network that affect epigenetic status. \n  \nBio: \nYuliang Wang is a Senior Research Associate in the Computational Biology Program\, School of Medicine at Oregon Health & Science University. He received his PhD in Chemical Engineering and MS in Applied Statistics from University of Illinois\, Urbana-Champaign in 2013. He did his postdoctoral training in computational biology at Sage Bionetworks in Seattle (2013-2014).\n\n
DTEND:20160201T110000
DTSTART:20160201T100000
LOCATION:Gates Commons\, 691 Paul G Allen Center for Computer Science & Engineering
SUMMARY:CSE Research Seminar: Yuliang Wang (UIUC/OHSU): Integrative genomic data analysis to understand interactions between metabolic\, transcriptional\, epigenetic states in stem cell biology
UID:20160201T100000c@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2853
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:Cities are increasingly publishing data about their operations while also internally using data to improve the effectiveness and quality of services through optimization\, predictive analytics\, and other methods.  This represents new opportunities for collaboration between cities\, national laboratories\, and universities in areas ranging from scalable data infrastructure to tools for data analytics\, along with challenges such as replicability of solutions between cities\, integrating and validating data for scientific investigation\, and protecting privacy.  For many urban questions\, new data sources will be required with greater spatial and/or temporal resolution\, driving innovation in the use of sensor in mobile devices as well as embedded sensing infrastructure in the built environment.  Catlett will discuss the work that Argonne National Laboratory and the University of Chicago are doing in partnership with the City of Chicago and other cities through the Urban Center for Computation and Data\, focusing on key scalable data infrastructure\, data analytics\, and resilient autonomous urban-scale sensor networks.\n  \nBio: \nCharlie Catlett is a Senior Computer Scientist at Argonne National Laboratory and a Senior Fellow at the Computation Institute of the University of Chicago and Argonne\, where he is founding director of the Urban Center for Computation and Data (UrbanCCD). He is also a Visiting Artist at the School of the Art Institute of Chicago.  Catlett is working with scientists from area universities and laboratories\, members of the technical community in the Chicago area and city policymakers to explore science-based approaches to opportunities and challenges of city design and operation.  \n  \nFrom 2007 to 2011 Catlett was the Chief Information Officer at Argonne\, and from 2004 to 2007 he was Director of the National Science Foundation's TeraGrid initiative - a nationally distributed supercomputing facility involving fifteen universities and federal laboratories. Before joining the UChicago and Argonne in 2000\, Catlett was Chief Technology Officer at the National Center for Supercomputing Applications at the University of Illinois at Urbana-Champaign.  At NCSA he participated\, beginning at NCSA's founding in 1985\, in the development of NSFNET\, one of several early national networks that evolved into what we now experience as the Internet. Following the release of NCSA's Mosaic web browser\, he and his team at NCSA helped develop and support scalable web server infrastructure.\n  \nFrom 1999 to 2004 Catlett directed the design and deployment of the I-WIRE optical network\, funded by the State of Illinois\, which connects research institutions in the Chicago area and downstate Illinois with a dedicated fiber optic network for research and education.   Catlett received a B.S. in Computer Engineering from the University of Illinois\, Urbana-Champaign in 1983.
DTEND:20160202T163000
DTSTART:20160202T153000
LOCATION:
SUMMARY:UW CSE Colloquium: Charlie  Catlett (Argonne National Laboratory and the University of Chicago): The Array of Things: Resilient\, Autonomous\, Urban-Scale Sensor Networks
UID:20160202T153000c@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2829
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:In robot collectives\, interactions between large numbers of individually simple robots lead to complex global behaviors. A great source of inspiration is social insects such as ants and bees\, where thousands of individuals coordinate to handle advanced tasks like food supply and nest construction in a remarkably scalable and error tolerant manner. Likewise\, robot swarms have the ability to address tasks beyond the reach of single robots\, and promise more efficient parallel operation and greater robustness due to redundancy. Key challenges involve both control and physical implementation. In this seminar I will discuss an approach to such systems relying on embodied intelligent robots designed as an integral part of their environment\, where passive mechanical features replace the need for complicated sensors and control.\n   The majority of my talk will focus on a team of robots for autonomous construction of user-specified three-dimensional structures developed during my thesis. Additionally\, I will give a brief overview of my research on the Namibian mound-building termites that inspired the robots. Finally\, I will talk about my current research thrust\, enabling stand-alone centimeter-scale soft robots to eventually be used in swarm robotics as well. My work advances the aim of collective robotic systems that achieve human-specified goals\, using biologically-inspired principles for robustness and scalability.\n  \nShort Bio:  \nKirstin Petersen is a Postdoc with the Physical Intelligence Department at the Max Planck Institute for Intelligent Systems in Germany. Here\, she develops novel soft actuators to enable stand-alone centimeter-scale soft robots. Kirstin finished her Ph.D. in computer science at Harvard University and the Wyss Institute for Biologically Inspired Engineering in 2014\, her thesis topic a combination of termite-inspired robots for collective construction and corresponding studies of the real termites. This work was featured in and on the cover of Science in February 2014\, and was elected among the top ten scientific breakthroughs of 2014. Kirstin completed her M.Sc. in modern artificial intelligence in 2008 and a B.Sc. in electro-technical engineering in 2005\, both with the University of Southern Denmark. In her spare time Kirstin is an avid hiker and traveler.
DTEND:20160209T163000
DTSTART:20160209T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Kirstin H Petersen (Harvard University/Max Planck Institute for Intelligent Systems): Designing Robot Collectives
UID:20160209T153000a@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2864
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:Most applications of machine learning across science and industry rely on the holdout method for model selection and validation. Unfortunately\, the holdout method often fails in the now common scenario where the analyst works interactively with the data\, iteratively choosing which methods to use by probing the same holdout data many times.\n  \nIn this talk\, we apply the principle of algorithmic stability to design reusable holdout methods\, which can be used many times without losing the guarantees of fresh data. Applications include a model benchmarking tool that detects and prevents overfitting at scale.\n  \nWe conclude with a bird's eye view of what algorithmic stability says about machine learning at large\, including new insights into stochastic gradient descent\, the most popular optimization method in contemporary machine learning.\n  \nShort bio: \nMoritz Hardt is a senior research scientist at Google Research. After obtaining a PhD in computer science from Princeton University in 2011\, he worked at IBM Research Almaden on algorithmic principles of machine learning. Then he moved to Google to join the Foundations of Applied Machine Learning group where his mission is to build guiding theory and scalable algorithms that make the practice of machine learning more reliable\, transparent\, and effective.\n
DTEND:20160211T163000
DTSTART:20160211T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Moritz Hardt (Princeton/Google Research): Overcoming Overfitting with Algorithmic Stability
UID:20160211T153000c@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2848
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:Moore's Law has historically been the driving force behind computing industry innovation. Today\, the computing industry as a whole is facing several challenges on all the key frontiers of microprocessor innovation and development. Generational improvements in processor performance\, continual reductions in its power consumption\, and sustained hardware execution reliability have all but reached a plateau\, if not begun to falter\, due to technology scaling issues. These challenges affect all domains of computing\, ranging from mobile devices to high-performance systems. This talk focuses specifically on end-user mobile computing devices\, outlining the challenges that face us in building future high-performance mobile devices that are responsive on a mobile power budget. By examining the mobile application software stack and the existing processor architectures that support it\, the talk presents how to bridge the increasing gap between end-user satisfaction and energy-efficient mobile computing. Domain-specific architectures\, coupled with feedback-directed runtime software optimization that is guided by programmer-driven annotations\, can enable us to build future high-performance\, energy-efficient mobile computing devices. Synergistic\, and systematic\, cross-layer optimization is a fundamental necessity for overcoming the performance\, power and reliability challenges that mark the "end" of the Moore's Law era and the beginning of the Moore's crawl period.\n  \nBiography: \nVijay Janapa Reddi is an Assistant Professor in the Department of Electrical and Computer Engineering at The University of Texas at Austin. His research interests include system architecture and software design and implementation to address performance\, power\, energy and reliability issues for mobile and high-performance computing systems. He is the recipient of the Intel Early Career Award\, PLDI Most Influential Paper Award\, and Best Paper and Top Picks awards in Computer Architecture. Beyond his research activities\, Vijay is very passionate about STEM education\, particularly involving computer science education starting at an early age. He is responsible for the Hands-On Computer Science (HaCS) curriculum that teaches computer science to middle school students in the Austin Independent School District (AISD) through Arduino-based hands-on projects. AISD ties directly into the heart of the public education system in Austin\, Texas. Vijay received his Ph.D. in Computer Science from Harvard University. He can be contacted at vj@ece.utexas.edu.\n
DTEND:20160218T163000
DTSTART:20160218T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Vijay Janapa Reddi (Harvard University/UT Austin): From Moore's Law to Moore's Crawl: Architecting the Next Generation of Mobile Computing Devices
UID:20160218T153000a@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2847
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:NOTE: This talk will NOT be broadcast live.  It will be taped only for internal use.\n  \nWe introduce a new kernel-based approach for the design of multi-layer convolutional feature representations for signals and images. The majority of deep convolutional neural networks (ConvNets) require supervision to learn state-of-the-art feature representations\, which can be problematic for real-world applications where supervision can be expensive or unconceivable. Unsupervised learning of feature representations is currently a major open problem. \n  \nThe proposed approach allow to define a feature representation that is based on a kernel (feature) map. The exact version of this feature representation is data-independent. An explicit kernel (feature) map can be computed to approximate it for computational efficiency\, without the need for supervision. We illustrate the potential of the approach on several applications: i) patch matching\; ii) image retrieval\; iii) image classification. We settle new state-of-the-art\, or compete on par with state-of-the-art methods\, on public benchmarks for all three computer vision tasks. \n  \nBio: \nZaid Harchaoui is currently Visiting Assistant Professor at the Courant Institute for Mathematical Sciences of NYU. Zaid was a permanent researcher at Inria from 2010 to 2015. He received his PhD from ParisTech (Paris\, France). He received the Inria award for scientific excellence and the NIPS reviewer award. He gave a tutorial on "Frank-Wolfe\, greedy algorithms\, and friends" at ICML'14\, on "Large-scale visual recognition" at CVPR'13\, and on "Machine Learning for Computer Vision" at MLSS Kyoto 2015. He recently co-organized the workshop on “Optimization for Machine Learning” at NIPS'14\, and the "Optimization and Statistical Learning" workshop in 2015 and 2013 in Ecole de Physique des Houches (France). He is associate editor of IEEE Signal Processing Letters. He is Area Chair for ICML 2015\, ICML 2016\, and NIPS 2016.  
DTEND:20160223T163000
DTSTART:20160223T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Zaid Harchaoui (Telecom Paris Tech/New York University): Towards Deep Convolutional Methods without Supervision
UID:20160223T153000a@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2859
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:Even though considered a rapid prototyping tool\, 3D printers are very slow. Many objects require several hours of printing time or even have to print overnight. One could argue that the way 3D printers are currently operated is similar to the batch processing of punched cards in the early days of computing: all input parameters are pre-defined in the 3D modeling stage\, the 3D printer then simply executes the instructions without human intervention. Since batch processing requires carefully thinking ahead\, it is limited to expert users who can reason about the consequences of their design decisions.\n\nIn the history of computing\, moving away from batch processing enabled completely new interaction paradigms: while batch processing required carefully thinking ahead\, command line input allowed for tighter feedback loops\, and direct manipulation finally enabled even novice users to quickly iterate towards a solution. I believe repeating this evolution for the editing of physical matter will enable novice users to build objects only trained experts can create today.\n\nIn this talk\, I show how technological advances in two areas lay the foundation for achieving this goal: First\, we need to develop tools that allow novice users to directly manipulate physical matter under computer control. Second\, since direct manipulation requires feedback in real-time after every editing step\, we need faster fabrication techniques that can accomplish instant physical change.\n\nBio\nStefanie Mueller is a PhD student working with Patrick Baudisch in the Human-Computer Interaction Lab at Hasso Plattner Institute. In her research\, she develops novel hardware and software systems that advance personal fabrication technologies. Stefanie has published 10 papers at the most selective HCI venues CHI and UIST\, for which she received a best paper award and two best paper nominees. She is also serving on the CHI and UIST program committees as an associate chair. In addition\, Stefanie has been an invited speaker at universities and research labs\, such as MIT\, CMU\, Cornell\, Microsoft Research\, Disney Research\, and Adobe Research.\n
DTEND:20160225T163000
DTSTART:20160225T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Stefanie Mueller (Hasso Plattner Institute): Interacting with Personal Fabrication Machines
UID:20160225T153000c@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2845
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:TBD
DTEND:20160301T163000
DTSTART:20160301T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium:  TBD (TBD): TBD
UID:20160301T153000a@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2846
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:I will present flexible algorithms for model discovery and model fitting which apply to broad\, open-ended classes of models\, yet which also incorporate model-specific algorithmic insights. First\, I will introduce a framework for building probabilistic models compositionally out of common modeling motifs\, such as clustering\, sparsity\, and dimensionality reduction. This compositional framework yields a variety of existing models as special cases. We can flexibly perform posterior inference across this large\, open-ended space of models by composing sophisticated inference algorithms carefully designed for the individual modeling motifs. An automatic structure search procedure over this space of models yields sensible analyses of datasets as diverse as motion capture\, natural image patches\, and Senate voting records\, all using a single software package with no hand-tuned metaparameters. Applying a similar compositional structure search procedure to Gaussian Process models yields interpretable decompositions of diverse time series datasets and enables automatic generation of natural language reports.  Finally\, compositional structure search depends crucially on the estimation of intractable likelihoods. I will briefly outline an approach for obtaining precise likelihood estimates with rigorous tail bounds by sandwiching the true value between stochastic upper and lower bounds.\n\nBio\nRoger Grosse is a Postdoctoral Fellow in the University of Toronto machine learning group. He received his Ph.D. in computer science from MIT under the supervision of of Bill Freeman. He is a recipient of the NDSEG Graduate Fellowship\, the Banting Postdoctoral Fellowship\, and outstanding paper awards at the International Conference of Machine Learning (ICML) and the Conference for Uncertainty in AI (UAI). He is also a co-creator of Metacademy\, an open-source web site for developing personalized learning plans in machine learning and related fields.
DTEND:20160303T163000
DTSTART:20160303T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Roger B Grosse (MIT/University of Toronto): Exploiting compositionality to explore a large space of model structures
UID:20160303T153000c@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2850
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:This talk will be taped for internal UW CSE faculty use only.  It will not be available for on-demand viewing.\n  \nFrequent headline-grabbing data breaches suggest that current best practices for safeguarding personal data are woefully inadequate.  To try to move beyond the cycle of attacks and patches we see today\, I design and build systems with formal end-to-end guarantees.  For example\, to provide strong guarantees for outsourced computations\, I developed a new cryptographic framework\, verifiable computation\, which allows clients to outsource general computations to completely untrusted services and efficiently verify the correctness of each returned result.  Through improvements to the theory and the underlying systems\, we reduced the costs of verification by over twenty orders of magnitude.  As a result\, verifiable computation is now a thriving research area that has produced several startups\, as well as enhancements to the security and privacy of X.509\, MapReduce\, and Bitcoin.\n\nWhile verifiable computation provides strong guarantees\, even the best cryptographic system is useless if implemented badly\, applied incorrectly\, or used in a vulnerable system.  Thus\, I have led a team of researchers and engineers in the Ironclad project\, working to expand formal software verification to provide end-to-end guarantees about the security and reliability of complex systems.  By creating a set of new tools and methodologies\, Ironclad produced the first complete stack of verified-secure software.  We also recently developed the first methodology for verifying both the safety and liveness of complex distributed systems implementations.  While interesting challenges remain\, I expect that verification will fundamentally improve the software that underpins our digital and physical infrastructure.\n\nBio\nBryan Parno is a Researcher in the Security and Privacy Group at Microsoft Research.  After receiving a Bachelor's degree from Harvard College\, he completed his PhD at Carnegie Mellon University\, where his dissertation won the 2010 ACM Doctoral Dissertation Award.  In 2011\, he was selected for Forbes' 30-Under-30 Science List.  He formalized and worked to optimize verifiable computation\, receiving a Best Paper Award at the IEEE Symposium on Security and Privacy his advances.  He coauthored a book on Bootstrapping Trust in Modern Computers\, and his work in that area has been incorporated into the latest security enhancements in Intel CPUs. His research into security for new application models was incorporated into Windows and received a Best Paper Awards at the IEEE Symposium on Security and Privacy and the USENIX Symposium on Networked Systems Design and Implementation.  He has recently extended his interest in bootstrapping trust to the problem of building practical\, formally verified secure systems. His other research interests include user authentication\, secure network protocols\, and security in constrained environments (e.g.\, RFID tags\, sensor networks\, or vehicles).
DTEND:20160310T163000
DTSTART:20160310T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Bryan Parno (Microsoft Research): Fully Verified Outsourced Computation
UID:20160310T153000d@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2844
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:NOTE: This talk will be taped only for internal UW CSE faculty use - it will not be broadcast live and it won't be available for viewing on-demand.\n  \nDatacenter workloads have demanding performance requirements\, including the simultaneous need for high throughput\, low tail latency\, and high server utilization. While modern hardware is compatible with these goals\, modern operating systems remain a bottleneck. Better OS abstractions could significantly improve performance\, yet deploying these abstractions has become intractable given the size and complexity of today's systems.\n  \nI will first discuss Dune\, a kernel extension that allows OS developers to sidestep software and hardware complexity by running an OS within an ordinary Linux process.  With Dune\, developers can both access the capabilities of raw hardware and fall back on the functionality of a full Linux environment where convenient. I will then discuss IX and Shinjuku\, two generations of new datacenter-focused operating systems that were enabled by Dune. IX provides a novel system call interface that greatly improves network throughput without sacrificing latency. For example\, IX improves Memcached's TCP throughput by 5x over Linux. Shinjuku\, an ongoing research effort\, aims to significantly increase CPU utilization through a centralized approach to intra-server load balancing.\n  \nBio: \nAdam Belay is a Ph.D. candidate in Computer Science at Stanford University\, where he is a member of the Secure Computer Systems Group and the Multiscale Architecture and Systems Team. Previously\, he worked on storage virtualization at VMware Inc. and contributed substantial power management code to the Linux Kernel project.  Adam's research area is operating systems and networking.  Much\nof his work has focused on restructuring computer systems so that developers can more easily reach the full performance potential of hardware. Adam has received a Stanford Graduate Fellowship\, a VMware Graduate Fellowship\, and an OSDI Jay Lepreau Best Paper Award.
DTEND:20160329T163000
DTSTART:20160329T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Adam Matthew Belay (Stanford University): Unleashing Hardware Potential through Better OS Abstractions
UID:20160329T153000b@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2855
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:NOTE: This talk will NOT be broadcast live!  It will be taped only for internal UW CSE faculty use.\n\nFuture visual computing applications -- from photorealistic real-time rendering\, to 4D light field cameras\, to pervasive sensing and computer vision-demand orders of magnitude more computation than we currently have. From data centers to mobile devices\, performance and energy scaling is limited by locality (the distance over which data has to move\, e.g.\, from nearby caches\, far away main memory\, or across networks) and parallelism. Because of this\, I argue that we should think of the performance and efficiency of an application as determined not just by the algorithm and the hardware on which it runs\, but critically also by the organization of computations and data. For algorithms with the same complexity -- even the exact same set of arithmetic operations and data executing on the same hardware\, the order and granularity of execution and placement of data can easily change performance by an order of magnitude because of locality and parallelism. To extract the full potential of our machines\, we must treat the organization of computation as a first class concern while working across all levels from algorithms and data structures\, to compilers\, to hardware.\n\nThis talk will present facets of this philosophy in systems I have built for visual computing applications from image processing and vision\, to 3D rendering\, simulation\, optimization\, and 3D printing. I will show that\, for data-parallel pipelines common in graphics\, imaging\, and other data-intensive applications\, the organization of computations and data for a given algorithm is constrained by a fundamental tension between parallelism\, locality\, and redundant computation of shared values. I will focus particularly on the Halide language and compiler for image processing\, which explicitly separates what computations define an algorithm from the choices of organization which determine parallelism\, locality\, memory footprint\, and synchronization. I will show how this approach can enable much simpler programs to deliver performance often many times faster than the best prior hand-tuned C\, assembly\, and CUDA implementations\, while scaling across radically different architectures\, from ARM cores\, to massively parallel GPUs\, to FPGAs and custom ASICs.\n\nBio\n\nJonathan Ragan-Kelley is a postdoc in computer science at Stanford. He works on high-efficiency visual computing\, including systems\, compilers\, and architectures for image processing\, vision\, 3D rendering\, 3D printing\, physical simulation\, and scientific computing. He earned his PhD in Computer Science at MIT in 2014\, where he built the Halide language for high-performance image processing. Halide is now used throughout industry to deploy code to hundreds of millions of smartphones and process tens of billions of images per day. Jonathan previously built the Lightspeed preview system\, which was used on over a dozen films at Industrial Light & Magic and was a finalist for an Academy Technical Achievement Award. He has worked in GPU architecture\, compilers\, and research at NVIDIA\, Intel\, and ATI.
DTEND:20160331T163000
DTSTART:20160331T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Jonathan Ragan-Kelley (MIT/Stanford): Organizing Computation for High-Performance Visual Computing
UID:20160331T153000c@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2858
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:TBD
DTEND:20160404T163000
DTSTART:20160404T153000
LOCATION:Gates Commons\, CSE 691 Paul G Allen Center for Computer Science & Engineering
SUMMARY:CSE Research Seminar:  TBD (TBD): TBD
UID:20160404T153000@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2879
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:The recent success of machine learning (ML) in both science and industry has generated an increasing demand to support ML algorithms at scale. In this talk\, I will discuss strategies to gracefully scale machine learning on modern parallel computational platforms. A common approach to such scaling is coordination-free parallel algorithms\, where individual processors run independently without communication\, thus maximizing the time they compute. However\, analyzing the performance of these algorithms can be challenging\, as they often introduce race conditions and synchronization problems.\n\nIn this talk\, I will introduce a general methodology for analyzing asynchronous parallel algorithms. The key idea is to model the effects of core asynchrony as noise in the algorithmic input.  This allows us to understand the performance of several popular asynchronous machine learning approaches\, and to determine when asynchrony effects might overwhelm them.  To overcome these effects\, I will propose a new framework for parallelizing ML algorithms\, where all memory conflicts and race conditions can be completely avoided. I will discuss the implementation of these ideas in practice\, and demonstrate that they outperform the state-of-the-art across a large number of ML tasks on gigabyte-scale data sets.\n\nBio\nDimitris Papailiopoulos is a postdoctoral researcher in the Department of Electrical Engineering and Computer Sciences at UC Berkeley and a member of the AMPLab. His research interests span machine learning\, coding theory\, and parallel and distributed algorithms\, with a current focus on coordination-free parallel machine learning\, large-scale data and graph analytics\, and the use of codes to speed up distributed computation. Dimitris completed his Ph.D. in electrical and computer engineering at UT Austin in 2014. At Austin he worked under the supervision of Alex Dimakis. In 2015\, he received the IEEE Signal Processing Society\, Young Author Best Paper Award.
DTEND:20160405T163000
DTSTART:20160405T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Dimitris Papailiopoulos (University of Texas\, Austin/UC Berkeley): Less Talking\, More Learning: Avoiding Coordination In Parallel Machine Learning Algorithms
UID:20160405T153000c@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2860
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:The purpose of obfuscation is to recompile programs in a way that preserves their functionality but otherwise renders their code unintelligible. Envisioned by Diffie and Hellman already in the 70's as a means of obtaining public-key encryption\, it is known by now that this concept may have far reaching implications to cryptography and complexity theory. In particular\, program obfuscation suggests (often exclusive) solutions to some of the most challenging privacy and security problems in the age of cloud computing and social networks.\n\nAt the same time\, program obfuscation has turned out to be  an evasive goal to achieve\, or to even meaningfully define. For a long time\, solutions have been confined to heuristics\, whereas attempts to achieve any sense of provable security have mostly led to impossibility results. This gloomy state dramatically changed in recent years\, when it was shown that a relatively weak notion called indistinguishability obfuscation may be within reach (so far\, based on strong computational assumptions) and still has the potential of realizing many dream applications.\n\nIn this talk\, I will review the different aspects of obfuscation\, including central notions\, limitations\, and feasibility. As a demonstration of the power of obfuscation\, I will present a recent implication [Bitansky-Paneth-Rosen\, FOCS15] that goes beyond cryptography into a fundamental problem in complexity and algorithmic game theory -- the hardness of finding a Nash equilibrium. I will conclude with the main open problems and challenges in the area of obfuscation.\n\nBio\nNir Bitansky is a postdoctoral associate at the cryptography group at MIT CSAIL. He earned his Ph.D. in computer science from Tel Aviv University in 2014. His research is centered around cryptography and its interplay with other areas of theoretical computer science.
DTEND:20160407T163000
DTSTART:20160407T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Nir Bitansky (Tel Aviv University/MIT): Program obfuscation: the power of unreadable code
UID:20160407T153000b@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2865
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:Convex optimization has been studied extensively and is a prominent tool in various areas such as combinatorial optimization\, data analysis\, operations research\, and scientific computing.  Each field has developed specialized tools including data structures\, sampling methods\, and dimension reduction. In the past several years\, I have been combining and improving the optimization techniques from different fields to design faster optimization algorithms. \n  \nIn this talk\, I will discuss my work in this direction and illustrate it through my results on linear programming and general convex optimization. In particular\, I will present a new algorithm for solving linear programs\, which gives the first improvement to the running time for linear programming in 25 years. Then\, I will present the first nearly cubic time algorithm for solving general convex optimization problems. Furthermore\, I will discuss how these two results can be used to improve the running time of many classical combinatorial problems such as maximum flow and submodular function minimization.\n  \nThis talk will assume no prior knowledge of optimization.\n  \nBio: \nYin Tat Lee is a Ph.D. candidate in the department of mathematics at the Massachusetts Institute of Technology. He is interested in designing faster algorithms\, particularly for problems in optimization. Since he began his Ph.D. in 2012\, he has combined ideas from continuous and discrete mathematics to substantially advance the state-of-the-art for solving many fundamental problems in computer science\, such as linear programming\, maximum flow\, and submodular function minimization. He has received a variety of awards\, including the Best Student Paper Award at FOCS 2015\, Best Paper Award at SODA 2014\, Best Paper Award and Best Student Paper Award at FOCS 2014\, and Notable Article in Computing in 2014 by Computing Reviews.
DTEND:20160412T163000
DTSTART:20160412T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Yin Tat Lee (MIT): Faster algorithms for fundamental convex problems and their applications in combinatorial optimization
UID:20160412T153000b@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2856
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:The goal of my research is to develop algorithmic and theoretical techniques that push highly agile robotic systems to the brink of their hardware limits while guaranteeing that they operate in a safe manner despite uncertainty in the environment and dynamics.\n  \nIn this talk\, I will describe my work on algorithms for the synthesis of feedback controllers that come with associated formal guarantees on the stability of the robot and show how these controllers and certificates of stability can be used for robust planning in environments previously unseen by the system. In order to make these results possible\, my work connects deeply to computational tools such as sums-of-squares (SOS) programming and semidefinite programming from the theory of mathematical optimization\, along with approaches from nonlinear control theory.\n  \nI will describe this work in the context of the problem of high-speed unmanned aerial vehicle (UAV) flight through cluttered environments previously unseen by the robot. In this context\, the tools I have developed allow us to guarantee that the robot will fly through its environment in a collision-free manner despite uncertainty in the dynamics (e.g.\, wind gusts or modeling errors). The resulting hardware demonstrations on a fixed-wing airplane constitute one of the first examples of provably safe and robust control for robotic systems with complex nonlinear dynamics that need to plan in realtime in environments with complex geometric constraints.\n  \nBio: \nAnirudha Majumdar is a Ph.D. candidate in the Electrical Engineering and Computer Science department at MIT. He is a member of the Robot Locomotion Group at the Computer Science and Artificial Intelligence Lab and is advised by Prof. Russ Tedrake. Ani received his undergraduate degree in Mechanical Engineering and Mathematics from the University of Pennsylvania\, where he was a member of the GRASP lab. His research is primarily in robotics: he works on algorithms for controlling highly dynamics robots such as unmanned aerial vehicles with formal guarantees on the safety of the system. Ani's research has been recognized by the Siebel Foundation Scholarship and the Best Conference Paper Award at the International Conference on Robotics and Automation (ICRA) 2013.\n
DTEND:20160414T163000
DTSTART:20160414T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Anirudha Majumdar (MIT): Control of agile robots in complex environments with formal safety guarantees
UID:20160414T153000a@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2874
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:At present\, user interfaces are typically created by designers for an idealized set of users and the most common interactive devices. In view of the growing number and diversity of new devices\, it becomes increasingly difficult to design for the many possible input/output characteristics and contexts of use\, and interfaces remain fairly limited in their ability to adapt to devices and their role in a user's larger task. In this talk\, I will describe how my research in human-computer interaction blurs the boundaries of interactive technologies and enables user interfaces to seamlessly grow\, with the help of users and crowds\, to take advantage of many devices and use contexts that are poorly supported by current design. For example\, W3Touch allows user interfaces to adapt to a large variety of touch devices based on user performance metrics and crowd data mining\; XDBrowser customizes existing single-device web interfaces for multi-device use based on user-defined cross-device design patterns\; and WearWrite allows a user to provide input and interact with a document using their smartwatch on one end of the interface\, and a crowd of writers to perform actions on the user's behalf using larger and more powerful devices on the other end of the interface. I will outline a research agenda that has the goal of making user interface design itself natural\, embedding human intelligence into interactive computing technologies\, and supporting evidence-based design using large-scale interaction data.\n  \nBio: \nMichael Nebeling is a Swiss NSF Advanced Postdoc.Mobility Fellow and a Visiting Researcher in the Human-Computer Interaction Institute at Carnegie Mellon University hosted by Anind Dey. Before coming to CMU in 2015\, he was a Senior Researcher and Lecturer at the Department of Computer Science at ETH Zurich\, where he obtained his PhD in 2012. His research interests are at the intersection of human-computer interaction\, user interface engineering\, ubiquitous computing\, and crowdsourcing. As part of his research\, he has created many systems to support the design and evaluation of rich\, context-aware and adaptive\, cross-device\, multi-touch and multi-modal gesture and speech interfaces\, and has received six Best Paper Awards and Honorable Mentions at premier venues in human-computer interaction and engineering\, including ACM CHI and ACM EICS. Michael is committed to promoting engineering research within the HCI community. He has been an Associate Chair for the CHI Technology\, Systems and Engineering subcommittee for CHI 2014-2016. He was EICS 2015 Papers co-chair and EICS 2014 Late-Breaking Results co-chair. He is on the EICS steering committee and also a Senior PC member for EICS 2016.\n
DTEND:20160419T163000
DTSTART:20160419T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Michael Nebeling (ETH Zurich/CMU): Multiple Devices + Crowds = Richer User Interfaces
UID:20160419T153000a@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2863
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:Improvements in the performance and energy consumption of general purpose processors have slowed dramatically over the last decade.  This is due to the combined effect of breakdowns in transistor scaling\, causing severe chip-level power limitations\, and monolithic and inefficient general purpose microarchitecture.\n  \nIn this work\, I propose and evaluate a concept called "behavioral specialization"\, where the design of general purpose processors is modularized by adding programmable offload engines\, each best-suited for different program behaviors or characteristics.  To explore this principle\, I designed a modular general-purpose core which transparently improves performance and energy efficiency by integer factors. I also extend these principles to create an architecture for highly-regular and parallelizable workloads for a further order of magnitude improvements.\n  \nI have discovered that a small number of exploitable program behaviors can cover a majority of applications\, that dataflow architectures become practical and useful in hybrid execution with a general purpose core\, and that programmable architectures can be competitive with domain specific alternatives on well-behaved workloads\, with only small area and negligible energy overheads.  Overall\, behavioral specialization causes disruptive change in microprocessor tradeoffs\, enabling mobile-class processor energy-efficiency with desktop-class performance.\n  \nBio: \nTony Nowatzki is a Ph.D. student in the Department of Computer Sciences at the University of Wisconsin-Madison advised by Karu Sankaralingam. His broad research interests include architecture and compiler codesign and mathematical modeling. He is recipient of a Google PhD Fellowship. His work has been recognized with a IEEE Micro Top Picks award\, a PLDI Distinguished Paper Award\, SIGARCH and SIGPLAN CACM Research Highlights nominations\, and an IEEE Best of CAL award. He was the lead author on a Synthesis Lecture on "Optimization and Mathematical Modeling in Computer Architecture."\n
DTEND:20160421T163000
DTSTART:20160421T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Tony Nowatzki (U Wisconsin\, Madison): Reviving General Purpose Computing with Architectural Specialization
UID:20160421T153000b@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2875
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:TBD
DTEND:20160426T163000
DTSTART:20160426T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Julia Rubin (University of Toronto/MIT): TBD
UID:20160426T153000@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2862
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:When real people interact with algorithms (e.g. in auctions\, crowdsourcing\, Bitcoin\, etc.)\, they impose additional objectives beyond simply that the algorithm is correct\, fast\, and uses little storage. People strategize during these interactions\, so algorithms deployed in these settings must be robust against potential manipulation. Additionally\, people prefer transparent interactions\, so these algorithms must also be as simple as possible. My research addresses these\, and other novel challenges that arise when algorithms interact with strategic agents.\n  \nIn this talk\, I will focus on one aspect of this agenda and present a new algorithmic framework to solve any optimization problem in a way that is robust to strategic manipulation. I will further apply this framework to extend Myerson's celebrated characterization of optimal single-item auctions to multiple items (Myerson 1981)\, design mechanisms for job scheduling (Nisan and Ronen 1999)\, and resolve other problems at the interface of algorithms and game theory.\n  \nFinally\, I will briefly show how strategic considerations motivate nice questions in "traditional" areas of algorithm design as well\, and present some of my work in online algorithms\, convex optimization\, and parallel algorithms.  \n  \nBio: \nMatt received his PhD in EECS from MIT in 2014\, where he was advised by Costis Daskalakis. He is now a postdoc at Princeton University in the Computer Science department. His research focuses on designing algorithms that address constraints imposed by the strategic nature of agents that interact with them. For his thesis work on these topics\, he received MIT's George M. Sprowls award and the SIGecom Doctoral Dissertation award. Matt received his B.A. in Math from Cornell University.
DTEND:20160428T163000
DTSTART:20160428T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Matt Weinberg (MIT/Princeton): Algorithms for Strategic Agents
UID:20160428T153000c@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2876
END:VEVENT
END:VCALENDAR

