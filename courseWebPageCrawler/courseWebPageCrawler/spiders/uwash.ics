BEGIN:VCALENDAR
VERSION:2.0
PRODID:Data::ICal 0.20
X-LIC-LOCATION:America/Los_Angeles
X-WR-CALNAME:UW CSE Colloquium Calendar
X-WR-TIMEZONE:America/Los_Angeles
BEGIN:VEVENT
DESCRIPTION:Surveys reveal that network outages are prevalent\, and that many outages take hours to resolve\, resulting in significant lost revenue. Many bugs are caused by errors in configuration files which are programmed using arcane\, low-level languages\, akin to machine code. Taking our cue from program and hardware verification\, we suggest fresh approaches.\nI will first describe a geometric model of network forwarding called Header Space. While header space analysis is similar to finite state machine verification\, we exploit domain-specific structure to scale better than off-the shelf model checkers.   Next\, I show how to exploit physical symmetry to scale network verification for large data centers. While Emerson and Sistla showed how to exploit symmetry for model checking in 1996\, they exploited symmetry on the logical Kripke structure.\n \nWhile the first part of the talk is about analysis\, I will then describe our work in synthesis.   I will set the stage by describing a new re-configurable router architecture we proposed called RMT. This has led to an emerging language for programming routers called P4 that promises to extend the boundaries of Software Designed Networks. I will then describe a synthesis problem for flexible routers\, akin to code generation (packet transactions) and new algorithmic questions related to synthesizing routes in the face of uncertainty.\n(With collaborators at Edinburgh\, MSR\, MIT\,Stanford\, and University of Washington.)\n  \nBIOGRAPHY\nGeorge Varghese received his Ph.D. in 1992 from MIT. From 1993-1999\, he was a professor at Washington University\, and at UCSD from 1999 to 2013. He was the Distinguished Visitor in the computer science department at Stanford University from 2010-2011. He joined Microsoft Research in 2012.\n  \nHis book "Network Algorithmics" was published in December 2004 by Morgan-Kaufman. In May 2004\, he co-founded NetSift\, which was acquired by Cisco Systems in 2005. With colleagues\, he has won best paper awards at SIGCOMM (2014)\, ANCS (2013)\, OSDI (2008)\, PODC (1996)\, and the IETF Applied Networking Prize (2013). He has won lifetime achievement awards in networking from the EE community (2014 Kobayashi Award) and the CS Community (2014 SIGCOMM Award).  He won the IIT Bombay Distinguished Alumni Award in 2015. 
DTEND:20160105T163000
DTSTART:20160105T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: George Varghese (Microsoft Research): Network Verification and Synthesis - When Hoare Meets Cerf 
UID:20160105T153000a@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2832
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:NOTE: Talk was live-broadcast only - no archive available!\n  \nThe ultimate goal of cancer biology is to enable the use of molecular profiles\, such as DNA\, RNA\, protein and epigenetic data\, of individual cancer patients for diagnosis and treatment. There is substantial need for better ways to choose chemotherapy drugs in individual cancer patients. For example\, patients over 65 with acute myeloid leukemia (AML)\, an aggressive blood cancer\, have no better prognosis today than they did in 1980. For a growing number of diseases\, there is a fair amount of data on molecular profiles from patients. The most important step necessary to realize the ultimate goal is to identify molecular markers (e.g.\, certain genes or mutations) in these molecular data that predict treatment outcomes\, such as response to each chemotherapy drug. However\, due to the high-dimensionality (i.e.\, the number of variables is much greater than the number of samples) along with potential biological or experimental confounders\, it is an open challenge to identify robust biomarkers that are replicated across different studies.\n  \nIn this talk\, I will present two distinct machine learning (ML) approaches to resolve this challenge. These methods learn the low-dimensional features that are likely to represent important molecular events in the disease process in an unsupervised fashion\, based on molecular profiles from multiple populations of cancer patients. I will present two applications of these two methods  -- AML\, and ovarian cancer. When the first method was applied to AML data in collaboration with UW Center for Cancer Innovation\, a novel molecular marker for topoisomerase inhibitors\, widely used chemotherapy drugs in AML treatment\, was revealed.  The other method applied to ovarian cancer data led to a potential molecular driver for an aggressive 'mesenchymal' subtype\, in collaboration with UW Pathology and UW Genome Sciences. These findings will lead to better ways to treat cancer.\n  \nThis research is funded by the American Cancer Society\, the National Science Foundation\, and the National Institutes of Health.\n  \nBio: \nProfessor Su-In Lee is an Assistant Professor in the Departments of Computer Science & Engineering and Genome Sciences at the University of Washington. She received her Ph.D. degree in Electrical Engineering from Stanford University in 2009. Before joining the UW in 2010\, she was a Visiting Assistant Professor in the Computational Biology Department at Carnegie Mellon University.\n  \nHer interest is in developing advanced machine learning (ML) algorithms to solve various important problems in biology and medicine. Her lab is currently funded by the American Cancer Society\, the National Institutes of Health\, the National Science Foundation\, the Institute of Translational Health Sciences and the Solid Tumor Translational Research.  \n 
DTEND:20160112T163000
DTSTART:20160112T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Su-In Lee (Assistant Professor joint with UW CSE and UW Genome Science): Big data approach to identify novel biomarkers in cancer
UID:20160112T153000a@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2793
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:Despite all of the recent progress in Natural Language Processing\, humans still have much to teach machines about language.  By combining the complementary strengths of human language aptitude with sheer computational horsepower\, we can learn important phenomena in a way that is sensitive to the cost of human expertise.  We focus our cooperative attention on the problem of annotating text corpora with the true (i.e.\, high quality) labels\, both for the benefit of supervised learning in NLP and to facilitate linguistic investigation.\n  \nIt is now common practice to reduce the cost of producing annotated corpora by crowdsourcing multiple redundant judgments and aggregating them -- for example\, using majority vote -- to produce good consensus labels. We improve the quality of consensus labels inferred from imperfect annotations in a number of ways.  First\, we show that\, contrary to popular preference\, generative annotation aggregation models tend to outperform conditional aggregation models.  We leverage this insight to develop CSLDA\, a novel annotation aggregation model that improves on the state of the art for a variety of annotation tasks.  Second\, for situations when data is not readily modeled generatively\, we introduce a conditional data modeling approach -- based on vector-space text representations -- that achieves state-of-the-art results on several unusual semantic annotation tasks.  Finally\, we introduce a family of models capable of aggregating heterogeneous annotations such as label frequencies and labeled features.  Continuing with the theme of human/machine cooperation\, we present a multi-annotator active learning algorithm for this model family that jointly selects an annotator\, data items\, and annotation type.\n   \nThis is joint work with Paul Felt (now at IBM)\, Kevin Seppi (BYU)\, Jordan Boyd-Graber (Colorado)\, Kevin Black (now at Xactware)\, and Robbie Haertel (now at Google).\n   \nBio:\nEric Ringger is a Research Scientist at Facebook in Seattle.  He is also an associate professor of computer science at Brigham Young University\, currently on leave.  His research passion is to facilitate human/machine cooperation to solve difficult learning and natural language problems\, such as revealing useful patterns in natural language data.  His research has contributed to NLP\, machine learning\, text mining\, and (most recently) search.\n
DTEND:20160119T163000
DTSTART:20160119T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Eric Ringger (Facebook): Humans and Machines Cooperating to Find the "Truth" about Language: Bayesian Methods for Annotation Aggregation
UID:20160119T153000a@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2819
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:It has been nearly four years since the first MOOCs (massive open online courses) were offered by Stanford University. MOOCs are now offered to tens of millions of learners worldwide\, by hundreds of top universities. MOOCs are no longer an experiment - the learning\, reach\, and value they offer are now a reality. I will show how MOOCs provide opportunities for open-ended projects\, intercultural learner interactions\, and collaborative learning. I will discuss some of data that we are collecting from MOOCs\, and what we are learning from these data about both courses and learners. I'll also describe data and examples regarding the kind of transformative impact that can be derived from providing millions of people with access to the world's best education.\n\nBio\nDaphne Koller is the President and Co-Founder of Coursera\, the largest open online education provider with more than 15 million registered learners worldwide. Daphne leads the growth and nurturing of Coursera's partnerships with over 130 universities and educational institutions. Previously\, she was the Rajeev Motwani Professor of Computer Science at Stanford University\, where she served on the faculty for 18 years. She is the author of over 180 refereed publications appearing in venues such as Science\, Cell\, and Nature Genetics. Daphne was recognized as one of TIME Magazine 's 100 most influential people in 2012 and Newsweek's 10 most important people in 2010. She has been honored with multiple awards and fellowships during her career including the Sloan Foundation Faculty Fellowship in 1996\, the ONR Young Investigator Award in 1998\, the IJCAI Computers and Thought Award in 2001\, and the MacArthur Foundation Fellowship in 2004. Daphne was inducted into the National Academy of Engineering in 2011 and elected a fellow of the American Academy of Arts and Sciences in 2014.
DTEND:20160121T163000
DTSTART:20160121T153000
LOCATION:EEB-105
SUMMARY:Ben Taskar Memorial Lecture: Daphne Koller (Coursera): MOOCs Turn 4: What Have We Learned?
UID:20160121T153000b@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2818
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:Advancements in semiconductor technology have brought smart micro-scale wireless sensors within reach for many applications. However\, power continues to be a critical obstacle\, as these devices must typically operate without a wired power connection for long periods.  Innovative ultra-low-power analog\, mixed-signal\, and RF circuit design is necessary to realize the promise of micro-scale ubiquitous wireless sensors. \n  \nIn this talk I will describe low-power circuits that address challenges at each stage in the signal chain of a wireless sensor: signal acquisition\, processing\, and communication. These will include efficient low-noise amplifiers for neural recording\, sensing of sub-picoampere currents\, and a low-power wireless transmitter.  At the signal processing stage\, I will describe our recent work in mixed-signal computation for machine learning\, including an analog deep machine learning engine that exhibits a 280x improvement in energy efficiency relative to an equivalent digital implementation. I will also discuss higher-level considerations relevant to selecting a computational strategy and broad design themes that have emerged throughout the course of this work.\n  \nBio: \nJeremy Holleman is an Associate Professor in the department of Electrical Engineering and Computer Science at the University of Tennessee\, Knoxville.  He received a Bachelor's degree in Electrical Engineering from Georgia Tech in 1997\, and the Master's and Ph.D. degrees in Electrical Engineering in 2006 and 2009\, both from the University of Washington.  He has previously worked for Data I/O and National Semiconductor. His research interests include low-power analog VLSI\, biomedical interfaces\, mixed-mode signal processing\, and neuromorphic engineering.
DTEND:20160128T163000
DTSTART:20160128T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Jeremy Holleman (University of Tennessee): Smart Silicon: Low-power Circuits for Intelligent Wireless Sensors
UID:20160128T153000c@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2843
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:Biomedical research is generating a huge amount of genomic data. Extracting knowledge from Big Data to guide novel biological discovery is critical to fully realize its potential. However\, high-dimensional genomic data is known for its heterogeneity and noise.  One important way to enrich for true signal is to examine its effects in different but complementary data types (e.g.\, mRNA expression\, histone modification\, and metabolite abundance). In this talk\, I will present my work on integrating multiple types of data to study how cellular metabolic\, transcriptional and epigenetic states interact in stem cell biology. First I will present a computational method that integrates gene expression\, metabolite abundance and flux\, and network topology to infer genome-scale cell type-specific metabolic networks. Then I will present a case study where the integration of gene expression and metabolite abundance data leads to the discovery of a metabolic enzyme controlling the level of repressive histone modifications\, which in turn regulate a set of genes critical for early human development. I will also present a new approach to identify genes that regulate lineage-specific differentiation by integrating gene expression and histone modification ChIP-seq data. In the future\, leveraging large genomic datasets from the ENCODE project\, NIH Roadmap Epigenomics project and the FANTOM5 project\, cell type-specific metabolic networks will be reconstructed as a framework to understand how developmental transcription factors affect metabolism\, the pattern of epigenetic modifications in the metabolic network\, and critical nodes in the metabolic network that affect epigenetic status. \n  \nBio: \nYuliang Wang is a Senior Research Associate in the Computational Biology Program\, School of Medicine at Oregon Health & Science University. He received his PhD in Chemical Engineering and MS in Applied Statistics from University of Illinois\, Urbana-Champaign in 2013. He did his postdoctoral training in computational biology at Sage Bionetworks in Seattle (2013-2014).\n\n
DTEND:20160201T110000
DTSTART:20160201T100000
LOCATION:Gates Commons\, 691 Paul G Allen Center for Computer Science & Engineering
SUMMARY:CSE Research Seminar: Yuliang Wang (UIUC/OHSU): Integrative genomic data analysis to understand interactions between metabolic\, transcriptional\, epigenetic states in stem cell biology
UID:20160201T100000c@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2853
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:Cities are increasingly publishing data about their operations while also internally using data to improve the effectiveness and quality of services through optimization\, predictive analytics\, and other methods.  This represents new opportunities for collaboration between cities\, national laboratories\, and universities in areas ranging from scalable data infrastructure to tools for data analytics\, along with challenges such as replicability of solutions between cities\, integrating and validating data for scientific investigation\, and protecting privacy.  For many urban questions\, new data sources will be required with greater spatial and/or temporal resolution\, driving innovation in the use of sensor in mobile devices as well as embedded sensing infrastructure in the built environment.  Catlett will discuss the work that Argonne National Laboratory and the University of Chicago are doing in partnership with the City of Chicago and other cities through the Urban Center for Computation and Data\, focusing on key scalable data infrastructure\, data analytics\, and resilient autonomous urban-scale sensor networks.\n  \nBio: \nCharlie Catlett is a Senior Computer Scientist at Argonne National Laboratory and a Senior Fellow at the Computation Institute of the University of Chicago and Argonne\, where he is founding director of the Urban Center for Computation and Data (UrbanCCD). He is also a Visiting Artist at the School of the Art Institute of Chicago.  Catlett is working with scientists from area universities and laboratories\, members of the technical community in the Chicago area and city policymakers to explore science-based approaches to opportunities and challenges of city design and operation.  \n  \nFrom 2007 to 2011 Catlett was the Chief Information Officer at Argonne\, and from 2004 to 2007 he was Director of the National Science Foundation's TeraGrid initiative - a nationally distributed supercomputing facility involving fifteen universities and federal laboratories. Before joining the UChicago and Argonne in 2000\, Catlett was Chief Technology Officer at the National Center for Supercomputing Applications at the University of Illinois at Urbana-Champaign.  At NCSA he participated\, beginning at NCSA's founding in 1985\, in the development of NSFNET\, one of several early national networks that evolved into what we now experience as the Internet. Following the release of NCSA's Mosaic web browser\, he and his team at NCSA helped develop and support scalable web server infrastructure.\n  \nFrom 1999 to 2004 Catlett directed the design and deployment of the I-WIRE optical network\, funded by the State of Illinois\, which connects research institutions in the Chicago area and downstate Illinois with a dedicated fiber optic network for research and education.   Catlett received a B.S. in Computer Engineering from the University of Illinois\, Urbana-Champaign in 1983.
DTEND:20160202T163000
DTSTART:20160202T153000
LOCATION:
SUMMARY:UW CSE Colloquium: Charlie  Catlett (Argonne National Laboratory and the University of Chicago): The Array of Things: Resilient\, Autonomous\, Urban-Scale Sensor Networks
UID:20160202T153000c@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2829
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:In robot collectives\, interactions between large numbers of individually simple robots lead to complex global behaviors. A great source of inspiration is social insects such as ants and bees\, where thousands of individuals coordinate to handle advanced tasks like food supply and nest construction in a remarkably scalable and error tolerant manner. Likewise\, robot swarms have the ability to address tasks beyond the reach of single robots\, and promise more efficient parallel operation and greater robustness due to redundancy. Key challenges involve both control and physical implementation. In this seminar I will discuss an approach to such systems relying on embodied intelligent robots designed as an integral part of their environment\, where passive mechanical features replace the need for complicated sensors and control.\n   The majority of my talk will focus on a team of robots for autonomous construction of user-specified three-dimensional structures developed during my thesis. Additionally\, I will give a brief overview of my research on the Namibian mound-building termites that inspired the robots. Finally\, I will talk about my current research thrust\, enabling stand-alone centimeter-scale soft robots to eventually be used in swarm robotics as well. My work advances the aim of collective robotic systems that achieve human-specified goals\, using biologically-inspired principles for robustness and scalability.\n  \nShort Bio:  \nKirstin Petersen is a Postdoc with the Physical Intelligence Department at the Max Planck Institute for Intelligent Systems in Germany. Here\, she develops novel soft actuators to enable stand-alone centimeter-scale soft robots. Kirstin finished her Ph.D. in computer science at Harvard University and the Wyss Institute for Biologically Inspired Engineering in 2014\, her thesis topic a combination of termite-inspired robots for collective construction and corresponding studies of the real termites. This work was featured in and on the cover of Science in February 2014\, and was elected among the top ten scientific breakthroughs of 2014. Kirstin completed her M.Sc. in modern artificial intelligence in 2008 and a B.Sc. in electro-technical engineering in 2005\, both with the University of Southern Denmark. In her spare time Kirstin is an avid hiker and traveler.
DTEND:20160209T163000
DTSTART:20160209T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Kirstin H Petersen (Harvard University/Max Planck Institute for Intelligent Systems): Designing Robot Collectives
UID:20160209T153000a@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2864
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:Most applications of machine learning across science and industry rely on the holdout method for model selection and validation. Unfortunately\, the holdout method often fails in the now common scenario where the analyst works interactively with the data\, iteratively choosing which methods to use by probing the same holdout data many times.\n  \nIn this talk\, we apply the principle of algorithmic stability to design reusable holdout methods\, which can be used many times without losing the guarantees of fresh data. Applications include a model benchmarking tool that detects and prevents overfitting at scale.\n  \nWe conclude with a bird's eye view of what algorithmic stability says about machine learning at large\, including new insights into stochastic gradient descent\, the most popular optimization method in contemporary machine learning.\n  \nShort bio: \nMoritz Hardt is a senior research scientist at Google Research. After obtaining a PhD in computer science from Princeton University in 2011\, he worked at IBM Research Almaden on algorithmic principles of machine learning. Then he moved to Google to join the Foundations of Applied Machine Learning group where his mission is to build guiding theory and scalable algorithms that make the practice of machine learning more reliable\, transparent\, and effective.\n
DTEND:20160211T163000
DTSTART:20160211T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Moritz Hardt (Princeton/Google Research): Overcoming Overfitting with Algorithmic Stability
UID:20160211T153000c@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2848
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:Moore's Law has historically been the driving force behind computing industry innovation. Today\, the computing industry as a whole is facing several challenges on all the key frontiers of microprocessor innovation and development. Generational improvements in processor performance\, continual reductions in its power consumption\, and sustained hardware execution reliability have all but reached a plateau\, if not begun to falter\, due to technology scaling issues. These challenges affect all domains of computing\, ranging from mobile devices to high-performance systems. This talk focuses specifically on end-user mobile computing devices\, outlining the challenges that face us in building future high-performance mobile devices that are responsive on a mobile power budget. By examining the mobile application software stack and the existing processor architectures that support it\, the talk presents how to bridge the increasing gap between end-user satisfaction and energy-efficient mobile computing. Domain-specific architectures\, coupled with feedback-directed runtime software optimization that is guided by programmer-driven annotations\, can enable us to build future high-performance\, energy-efficient mobile computing devices. Synergistic\, and systematic\, cross-layer optimization is a fundamental necessity for overcoming the performance\, power and reliability challenges that mark the "end" of the Moore's Law era and the beginning of the Moore's crawl period.\n  \nBiography: \nVijay Janapa Reddi is an Assistant Professor in the Department of Electrical and Computer Engineering at The University of Texas at Austin. His research interests include system architecture and software design and implementation to address performance\, power\, energy and reliability issues for mobile and high-performance computing systems. He is the recipient of the Intel Early Career Award\, PLDI Most Influential Paper Award\, and Best Paper and Top Picks awards in Computer Architecture. Beyond his research activities\, Vijay is very passionate about STEM education\, particularly involving computer science education starting at an early age. He is responsible for the Hands-On Computer Science (HaCS) curriculum that teaches computer science to middle school students in the Austin Independent School District (AISD) through Arduino-based hands-on projects. AISD ties directly into the heart of the public education system in Austin\, Texas. Vijay received his Ph.D. in Computer Science from Harvard University. He can be contacted at vj@ece.utexas.edu.\n
DTEND:20160218T163000
DTSTART:20160218T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Vijay Janapa Reddi (Harvard University/UT Austin): From Moore's Law to Moore's Crawl: Architecting the Next Generation of Mobile Computing Devices
UID:20160218T153000a@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2847
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:NOTE: This talk will NOT be broadcast live.  It will be taped only for internal use.\n  \nWe introduce a new kernel-based approach for the design of multi-layer convolutional feature representations for signals and images. The majority of deep convolutional neural networks (ConvNets) require supervision to learn state-of-the-art feature representations\, which can be problematic for real-world applications where supervision can be expensive or unconceivable. Unsupervised learning of feature representations is currently a major open problem. \n  \nThe proposed approach allow to define a feature representation that is based on a kernel (feature) map. The exact version of this feature representation is data-independent. An explicit kernel (feature) map can be computed to approximate it for computational efficiency\, without the need for supervision. We illustrate the potential of the approach on several applications: i) patch matching\; ii) image retrieval\; iii) image classification. We settle new state-of-the-art\, or compete on par with state-of-the-art methods\, on public benchmarks for all three computer vision tasks. \n  \nBio: \nZaid Harchaoui is currently Visiting Assistant Professor at the Courant Institute for Mathematical Sciences of NYU. Zaid was a permanent researcher at Inria from 2010 to 2015. He received his PhD from ParisTech (Paris\, France). He received the Inria award for scientific excellence and the NIPS reviewer award. He gave a tutorial on "Frank-Wolfe\, greedy algorithms\, and friends" at ICML'14\, on "Large-scale visual recognition" at CVPR'13\, and on "Machine Learning for Computer Vision" at MLSS Kyoto 2015. He recently co-organized the workshop on “Optimization for Machine Learning” at NIPS'14\, and the "Optimization and Statistical Learning" workshop in 2015 and 2013 in Ecole de Physique des Houches (France). He is associate editor of IEEE Signal Processing Letters. He is Area Chair for ICML 2015\, ICML 2016\, and NIPS 2016.  
DTEND:20160223T163000
DTSTART:20160223T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Zaid Harchaoui (Telecom Paris Tech/New York University): Towards Deep Convolutional Methods without Supervision
UID:20160223T153000a@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2859
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:Even though considered a rapid prototyping tool\, 3D printers are very slow. Many objects require several hours of printing time or even have to print overnight. One could argue that the way 3D printers are currently operated is similar to the batch processing of punched cards in the early days of computing: all input parameters are pre-defined in the 3D modeling stage\, the 3D printer then simply executes the instructions without human intervention. Since batch processing requires carefully thinking ahead\, it is limited to expert users who can reason about the consequences of their design decisions.\n\nIn the history of computing\, moving away from batch processing enabled completely new interaction paradigms: while batch processing required carefully thinking ahead\, command line input allowed for tighter feedback loops\, and direct manipulation finally enabled even novice users to quickly iterate towards a solution. I believe repeating this evolution for the editing of physical matter will enable novice users to build objects only trained experts can create today.\n\nIn this talk\, I show how technological advances in two areas lay the foundation for achieving this goal: First\, we need to develop tools that allow novice users to directly manipulate physical matter under computer control. Second\, since direct manipulation requires feedback in real-time after every editing step\, we need faster fabrication techniques that can accomplish instant physical change.\n\nBio\nStefanie Mueller is a PhD student working with Patrick Baudisch in the Human-Computer Interaction Lab at Hasso Plattner Institute. In her research\, she develops novel hardware and software systems that advance personal fabrication technologies. Stefanie has published 10 papers at the most selective HCI venues CHI and UIST\, for which she received a best paper award and two best paper nominees. She is also serving on the CHI and UIST program committees as an associate chair. In addition\, Stefanie has been an invited speaker at universities and research labs\, such as MIT\, CMU\, Cornell\, Microsoft Research\, Disney Research\, and Adobe Research.\n
DTEND:20160225T163000
DTSTART:20160225T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Stefanie Mueller (Hasso Plattner Institute): Interacting with Personal Fabrication Machines
UID:20160225T153000c@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2845
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:TBD
DTEND:20160301T163000
DTSTART:20160301T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium:  TBD (TBD): TBD
UID:20160301T153000a@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2846
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:I will present flexible algorithms for model discovery and model fitting which apply to broad\, open-ended classes of models\, yet which also incorporate model-specific algorithmic insights. First\, I will introduce a framework for building probabilistic models compositionally out of common modeling motifs\, such as clustering\, sparsity\, and dimensionality reduction. This compositional framework yields a variety of existing models as special cases. We can flexibly perform posterior inference across this large\, open-ended space of models by composing sophisticated inference algorithms carefully designed for the individual modeling motifs. An automatic structure search procedure over this space of models yields sensible analyses of datasets as diverse as motion capture\, natural image patches\, and Senate voting records\, all using a single software package with no hand-tuned metaparameters. Applying a similar compositional structure search procedure to Gaussian Process models yields interpretable decompositions of diverse time series datasets and enables automatic generation of natural language reports.  Finally\, compositional structure search depends crucially on the estimation of intractable likelihoods. I will briefly outline an approach for obtaining precise likelihood estimates with rigorous tail bounds by sandwiching the true value between stochastic upper and lower bounds.\n\nBio\nRoger Grosse is a Postdoctoral Fellow in the University of Toronto machine learning group. He received his Ph.D. in computer science from MIT under the supervision of of Bill Freeman. He is a recipient of the NDSEG Graduate Fellowship\, the Banting Postdoctoral Fellowship\, and outstanding paper awards at the International Conference of Machine Learning (ICML) and the Conference for Uncertainty in AI (UAI). He is also a co-creator of Metacademy\, an open-source web site for developing personalized learning plans in machine learning and related fields.
DTEND:20160303T163000
DTSTART:20160303T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Roger B Grosse (MIT/University of Toronto): Exploiting compositionality to explore a large space of model structures
UID:20160303T153000c@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2850
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:Frequent headline-grabbing data breaches suggest that current best practices for safeguarding personal data are woefully inadequate.  To try to move beyond the cycle of attacks and patches we see today\, I design and build systems with formal end-to-end guarantees.  For example\, to provide strong guarantees for outsourced computations\, I developed a new cryptographic framework\, verifiable computation\, which allows clients to outsource general computations to completely untrusted services and efficiently verify the correctness of each returned result.  Through improvements to the theory and the underlying systems\, we reduced the costs of verification by over twenty orders of magnitude.  As a result\, verifiable computation is now a thriving research area that has produced several startups\, as well as enhancements to the security and privacy of X.509\, MapReduce\, and Bitcoin.\n\nWhile verifiable computation provides strong guarantees\, even the best cryptographic system is useless if implemented badly\, applied incorrectly\, or used in a vulnerable system.  Thus\, I have led a team of researchers and engineers in the Ironclad project\, working to expand formal software verification to provide end-to-end guarantees about the security and reliability of complex systems.  By creating a set of new tools and methodologies\, Ironclad produced the first complete stack of verified-secure software.  We also recently developed the first methodology for verifying both the safety and liveness of complex distributed systems implementations.  While interesting challenges remain\, I expect that verification will fundamentally improve the software that underpins our digital and physical infrastructure.\n\nBio\nBryan Parno is a Researcher in the Security and Privacy Group at Microsoft Research.  After receiving a Bachelor's degree from Harvard College\, he completed his PhD at Carnegie Mellon University\, where his dissertation won the 2010 ACM Doctoral Dissertation Award.  In 2011\, he was selected for Forbes' 30-Under-30 Science List.  He formalized and worked to optimize verifiable computation\, receiving a Best Paper Award at the IEEE Symposium on Security and Privacy his advances.  He coauthored a book on Bootstrapping Trust in Modern Computers\, and his work in that area has been incorporated into the latest security enhancements in Intel CPUs. His research into security for new application models was incorporated into Windows and received a Best Paper Awards at the IEEE Symposium on Security and Privacy and the USENIX Symposium on Networked Systems Design and Implementation.  He has recently extended his interest in bootstrapping trust to the problem of building practical\, formally verified secure systems. His other research interests include user authentication\, secure network protocols\, and security in constrained environments (e.g.\, RFID tags\, sensor networks\, or vehicles).
DTEND:20160310T163000
DTSTART:20160310T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Bryan Parno (Microsoft Research): Fully Verified Outsourced Computation
UID:20160310T153000c@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2844
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:TBD
DTEND:20160329T163000
DTSTART:20160329T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium:  TBD (TBD): TBD
UID:20160329T153000@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2855
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:NOTE: This talk will NOT be broadcast live!\nTBD
DTEND:20160331T163000
DTSTART:20160331T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Jonathan Ragan-Kelley (MIT/Stanford): TBD
UID:20160331T153000a@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2858
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:The recent success of machine learning (ML) in both science and industry has generated an increasing demand to support ML algorithms at scale. In this talk\, I will discuss strategies to gracefully scale machine learning on modern parallel computational platforms. A common approach to such scaling is coordination-free parallel algorithms\, where individual processors run independently without communication\, thus maximizing the time they compute. However\, analyzing the performance of these algorithms can be challenging\, as they often introduce race conditions and synchronization problems.\n\nIn this talk\, I will introduce a general methodology for analyzing asynchronous parallel algorithms. The key idea is to model the effects of core asynchrony as noise in the algorithmic input.  This allows us to understand the performance of several popular asynchronous machine learning approaches\, and to determine when asynchrony effects might overwhelm them.  To overcome these effects\, I will propose a new framework for parallelizing ML algorithms\, where all memory conflicts and race conditions can be completely avoided. I will discuss the implementation of these ideas in practice\, and demonstrate that they outperform the state-of-the-art across a large number of ML tasks on gigabyte-scale data sets.\n\nBio\nDimitris Papailiopoulos is a postdoctoral researcher in the Department of Electrical Engineering and Computer Sciences at UC Berkeley and a member of the AMPLab. His research interests span machine learning\, coding theory\, and parallel and distributed algorithms\, with a current focus on coordination-free parallel machine learning\, large-scale data and graph analytics\, and the use of codes to speed up distributed computation. Dimitris completed his Ph.D. in electrical and computer engineering at UT Austin in 2014. At Austin he worked under the supervision of Alex Dimakis. In 2015\, he received the IEEE Signal Processing Society\, Young Author Best Paper Award.
DTEND:20160405T163000
DTSTART:20160405T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Dimitris Papailiopoulos (University of Texas\, Austin/UC Berkeley): Less Talking\, More Learning: Avoiding Coordination In Parallel Machine Learning Algorithms
UID:20160405T153000c@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2860
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:The purpose of obfuscation is to recompile programs in a way that preserves their functionality but otherwise renders their code unintelligible. Envisioned by Diffie and Hellman already in the 70's as a means of obtaining public-key encryption\, it is known by now that this concept may have far reaching implications to cryptography and complexity theory. In particular\, program obfuscation suggests (often exclusive) solutions to some of the most challenging privacy and security problems in the age of cloud computing and social networks.\n\nAt the same time\, program obfuscation has turned out to be  an evasive goal to achieve\, or to even meaningfully define. For a long time\, solutions have been confined to heuristics\, whereas attempts to achieve any sense of provable security have mostly led to impossibility results. This gloomy state dramatically changed in recent years\, when it was shown that a relatively weak notion called indistinguishability obfuscation may be within reach (so far\, based on strong computational assumptions) and still has the potential of realizing many dream applications.\n\nIn this talk\, I will review the different aspects of obfuscation\, including central notions\, limitations\, and feasibility. As a demonstration of the power of obfuscation\, I will present a recent implication [Bitansky-Paneth-Rosen\, FOCS15] that goes beyond cryptography into a fundamental problem in complexity and algorithmic game theory -- the hardness of finding a Nash equilibrium. I will conclude with the main open problems and challenges in the area of obfuscation.\n\nBio\nNir Bitansky is a postdoctoral associate at the cryptography group at MIT CSAIL. He earned his Ph.D. in computer science from Tel Aviv University in 2014. His research is centered around cryptography and its interplay with other areas of theoretical computer science.
DTEND:20160407T163000
DTSTART:20160407T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Nir Bitansky (Tel Aviv University/MIT): Program obfuscation: the power of unreadable code
UID:20160407T153000b@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2865
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:Convex optimization has been studied extensively and is a prominent tool in various areas such as combinatorial optimization\, data analysis\, operations research\, and scientific computing.  Each field has developed specialized tools including data structures\, sampling methods\, and dimension reduction. In the past several years\, I have been combining and improving the optimization techniques from different fields to design faster optimization algorithms. \n  \nIn this talk\, I will discuss my work in this direction and illustrate it through my results on linear programming and general convex optimization. In particular\, I will present a new algorithm for solving linear programs\, which gives the first improvement to the running time for linear programming in 25 years. Then\, I will present the first nearly cubic time algorithm for solving general convex optimization problems. Furthermore\, I will discuss how these two results can be used to improve the running time of many classical combinatorial problems such as maximum flow and submodular function minimization.\n  \nThis talk will assume no prior knowledge of optimization.\n  \nBio: \nYin Tat Lee is a Ph.D. candidate in the department of mathematics at the Massachusetts Institute of Technology. He is interested in designing faster algorithms\, particularly for problems in optimization. Since he began his Ph.D. in 2012\, he has combined ideas from continuous and discrete mathematics to substantially advance the state-of-the-art for solving many fundamental problems in computer science\, such as linear programming\, maximum flow\, and submodular function minimization. He has received a variety of awards\, including the Best Student Paper Award at FOCS 2015\, Best Paper Award at SODA 2014\, Best Paper Award and Best Student Paper Award at FOCS 2014\, and Notable Article in Computing in 2014 by Computing Reviews.
DTEND:20160412T163000
DTSTART:20160412T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Yin Tat Lee (MIT): Faster algorithms for fundamental convex problems and their applications in combinatorial optimization
UID:20160412T153000b@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2856
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:The goal of my research is to develop algorithmic and theoretical techniques that push highly agile robotic systems to the brink of their hardware limits while guaranteeing that they operate in a safe manner despite uncertainty in the environment and dynamics.\n  \nIn this talk\, I will describe my work on algorithms for the synthesis of feedback controllers that come with associated formal guarantees on the stability of the robot and show how these controllers and certificates of stability can be used for robust planning in environments previously unseen by the system. In order to make these results possible\, my work connects deeply to computational tools such as sums-of-squares (SOS) programming and semidefinite programming from the theory of mathematical optimization\, along with approaches from nonlinear control theory.\n  \nI will describe this work in the context of the problem of high-speed unmanned aerial vehicle (UAV) flight through cluttered environments previously unseen by the robot. In this context\, the tools I have developed allow us to guarantee that the robot will fly through its environment in a collision-free manner despite uncertainty in the dynamics (e.g.\, wind gusts or modeling errors). The resulting hardware demonstrations on a fixed-wing airplane constitute one of the first examples of provably safe and robust control for robotic systems with complex nonlinear dynamics that need to plan in realtime in environments with complex geometric constraints.\n  \nBio: \nAnirudha Majumdar is a Ph.D. candidate in the Electrical Engineering and Computer Science department at MIT. He is a member of the Robot Locomotion Group at the Computer Science and Artificial Intelligence Lab and is advised by Prof. Russ Tedrake. Ani received his undergraduate degree in Mechanical Engineering and Mathematics from the University of Pennsylvania\, where he was a member of the GRASP lab. His research is primarily in robotics: he works on algorithms for controlling highly dynamics robots such as unmanned aerial vehicles with formal guarantees on the safety of the system. Ani's research has been recognized by the Siebel Foundation Scholarship and the Best Conference Paper Award at the International Conference on Robotics and Automation (ICRA) 2013.\n
DTEND:20160414T163000
DTSTART:20160414T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Anirudha Majumdar (MIT): Control of agile robots in complex environments with formal safety guarantees
UID:20160414T153000a@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2874
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:At present\, user interfaces are typically created by designers for an idealized set of users and the most common interactive devices. In view of the growing number and diversity of new devices\, it becomes increasingly difficult to design for the many possible input/output characteristics and contexts of use\, and interfaces remain fairly limited in their ability to adapt to devices and their role in a user's larger task. In this talk\, I will describe how my research in human-computer interaction blurs the boundaries of interactive technologies and enables user interfaces to seamlessly grow\, with the help of users and crowds\, to take advantage of many devices and use contexts that are poorly supported by current design. For example\, W3Touch allows user interfaces to adapt to a large variety of touch devices based on user performance metrics and crowd data mining\; XDBrowser customizes existing single-device web interfaces for multi-device use based on user-defined cross-device design patterns\; and WearWrite allows a user to provide input and interact with a document using their smartwatch on one end of the interface\, and a crowd of writers to perform actions on the user's behalf using larger and more powerful devices on the other end of the interface. I will outline a research agenda that has the goal of making user interface design itself natural\, embedding human intelligence into interactive computing technologies\, and supporting evidence-based design using large-scale interaction data.\n  \nBio: \nMichael Nebeling is a Swiss NSF Advanced Postdoc.Mobility Fellow and a Visiting Researcher in the Human-Computer Interaction Institute at Carnegie Mellon University hosted by Anind Dey. Before coming to CMU in 2015\, he was a Senior Researcher and Lecturer at the Department of Computer Science at ETH Zurich\, where he obtained his PhD in 2012. His research interests are at the intersection of human-computer interaction\, user interface engineering\, ubiquitous computing\, and crowdsourcing. As part of his research\, he has created many systems to support the design and evaluation of rich\, context-aware and adaptive\, cross-device\, multi-touch and multi-modal gesture and speech interfaces\, and has received six Best Paper Awards and Honorable Mentions at premier venues in human-computer interaction and engineering\, including ACM CHI and ACM EICS. Michael is committed to promoting engineering research within the HCI community. He has been an Associate Chair for the CHI Technology\, Systems and Engineering subcommittee for CHI 2014-2016. He was EICS 2015 Papers co-chair and EICS 2014 Late-Breaking Results co-chair. He is on the EICS steering committee and also a Senior PC member for EICS 2016.\n
DTEND:20160419T163000
DTSTART:20160419T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Michael Nebeling (ETH Zurich/CMU): Multiple Devices + Crowds = Richer User Interfaces
UID:20160419T153000a@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2863
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:TBD
DTEND:20160421T163000
DTSTART:20160421T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Tony Nowatzki (): TBD
UID:20160421T153000a@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2875
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:TBD
DTEND:20160426T163000
DTSTART:20160426T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Julia Rubin (University of Toronto/MIT): TBD
UID:20160426T153000@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2862
END:VEVENT
BEGIN:VEVENT
DESCRIPTION:When real people interact with algorithms (e.g. in auctions\, crowdsourcing\, Bitcoin\, etc.)\, they impose additional desiderata beyond simply that the algorithm is correct\, fast\, and uses little storage. People strategize during these interactions\, so algorithms deployed in these settings must be robust against strategic manipulation. Additionally\, people prefer transparent interactions\, so these algorithms must also be as simple as possible. My research addresses these\, and other novel challenges that arise when algorithms interact with strategic agents.\n  \nIn this talk\, I will focus on robustness against strategic manipulation\, and present a new algorithmic framework for these settings. Specifically\, I will present a black-box reduction from solving any optimization problem in strategic settings\, where the input is held by selfish agents with interests of their own\, to solving a perturbed version of that same optimization problem in traditional settings\, where the input is directly given. I will further apply this framework to resolve two longstanding open problems: extending Myerson's celebrated characterization of optimal single-item auctions to multiple items (Myerson 1981)\, and designing truthful mechanisms for job scheduling on unrelated machines (Nisan and Ronen 1999). \n  \nFinally\, I will briefly show how strategic considerations motivate nice questions in "traditional" areas of algorithm design as well\, and present some of my work in convex optimization\, parallel algorithms\, and prophet inequalities. \n  \nBio: \nMatt received his PhD in EECS from MIT in 2014\, where he was advised by Costis Daskalakis. He is now a postdoc at Princeton University in the Computer Science department. His research focuses on designing algorithms that address constraints imposed by the strategic nature of agents that interact with them. For his thesis work on these topics\, he received MIT's George M. Sprowls award and the SIGecom Doctoral Dissertation award. Matt received his B.A. in Math from Cornell University.
DTEND:20160428T163000
DTSTART:20160428T153000
LOCATION:EEB-105
SUMMARY:UW CSE Colloquium: Matt Weinberg (MIT/Princeton): Algorithms for Strategic Agents
UID:20160428T153000b@colloquia.cs.washington.edu
URL:http://www.cs.washington.edu/htbin-post/mvis/mvis?ID=2876
END:VEVENT
END:VCALENDAR

